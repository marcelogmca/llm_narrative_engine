16. modules/chaptermanagement.js

   * Purpose: Manages the storage and retrieval of chat messages as "chapters" in a Markdown file. It can
     append messages, retrieve all messages, and categorize them into "full," "summary," and "synopsis"
     chapters based on length and recency.
     Crucially, this module can handle not just plain text, but also a structured format that pairs each message 
     with an arbitrary JSON properties object. This allows for rich metadata to be associated with every turn, 
     such as feedback from other systems, game state snapshots, or character relationship scores. It forms the persistent, 
     on-disk "memory" of the story's evolution.
   * File Format: The chapter file is a Markdown file where each entry is separated by a special delimiter. 
   Each entry contains two primary XML-like tags:
       * <text>...</text>: Holds the raw story/dialogue content for that turn. Within this, user and assistant messages are wrapped in `<user>...</user>` and `<assistant>...</assistant>` tags respectively.
       * <properties>...</properties>: Holds a stringified JSON object. This can contain any key-value pairs. 
         For example, it is used to store { "writerfeedback": "...", "directorfeedback": "..." } from the orchestrator module.
   * Main Functions:
       * setChapterFile(filePath): Sets the target file path for chapter operations.
       * ensureFileSet(): Internal utility to ensure a chapter file path has been set.
       * cleanDoubleTags(xmlString): Utility function to remove consecutively nested identical XML-like tags (e.g., `<tag><tag>content</tag></tag>` becomes `<tag>content</tag>`).
       * appendMessage(message, auxobj): Appends a new entry to the chapter file. It takes the combined user prompt and assistant response (separated by `<<$>>`), wraps them in `<user>` and `<assistant>` tags within a `<text>` block, and serializes the `auxobj` into a `<properties>` tag. It also cleans double tags.
       * getChapters(): Reads the entire chapter file and parses it. It extracts the text content and the JSON properties object for each entry, returning a structured object `{ chapters, properties }`.
       * getChapterCount(): Retrieves the total number of messages (chapters) in the chapter file.
       * retrieveDatedChapters(): Categorizes chapters into recent "full," older "summary," and oldest
         "synopsis" sections, returning them along with their associated properties.
       * configure(newConfig): Updates module configuration (e.g., `FULL_CHAPTERS`, `SUMMARY_CHAPTERS`, `CHAT_SEPARATOR`).
       * CONFIG: The configuration object itself, directly exported for read access by other modules.
   * Modularity: Good. Focused on chapter file management.
   * Dependencies: fs, path, utils.js.
   * Flow Analysis:
       1. Initialization: `main.js` directly imports and configures `chaptermanagement` in `initializeNarrativeEngine` and uses it in `vnHandlers.generateVNTurn` and `chapterHandlers.saveMessageChapter`.
       2. Message Saving: `main.js` (triggered by `save-message-chapter` from `renderer_vn.js` or `generate-vn-turn` within `main.js`) calls
          `chaptermanagement.appendMessage()` to save LLM responses and associated metadata.
       3. Chapter Retrieval: `prompt_builder.js` calls `chaptermanagement.retrieveDatedChapters()` to get
          historical chat context for the LLM, including any orchestrator feedback stored in properties.

  17. modules/narrativeEngine.js (Narrative Generation Engine)


   * Purpose: This module is the central orchestrator for generating the core narrative text using LLMs. It integrates prompt building, orchestrator feedback, and handles the main LLM calls.
   * Main Functions:
       * generateNextChapter(files, prompt, rootDirectory, projectName, isObsidianMode): The primary function responsible for:
           * Calling `prompt_builder.buildMessages` to construct the LLM prompt, incorporating selected files, user prompt, and historical chapter data.
           * Injecting `writerFeedback` from previous orchestrator runs into the prompt to guide the LLM.
           * Executing the Turn 0 orchestrator (`orchestrator.runTurn0`) if it's the beginning of a new story, and injecting its `writerfeedback`.
           * Making the final LLM call to OpenRouter to generate the next chapter's text.
           * Returning the generated text and any orchestrator feedback (especially Turn 0 feedback) for persistence.
       * configure(config): Sets up necessary dependencies like `chaptermanagementInstance`, `getChatFilePathFromRenderer`, and `searchStringHandler`.)
   * Modularity: High. It orchestrates the narrative generation process, delegating prompt construction to `prompt_builder` and orchestrator logic to `orchestrator`.
   * Dependencies: utils.js (for Logger, openrouterRequest, readSettings), prompt_builder.js, fs/promises, obsidianmode/utils/cache_manager.js (for `baseCacheDir`), orchestrator.js, path.
   * Flow Analysis:
       1. Trigger: Called by `main.js` (from `fileHandlers.sendToLLM` for text generation tab, or `vnHandlers.generateVNTurn` for VN tab).
       2. Prompt Building: Calls `prompt_builder.buildMessages` to get the structured messages for the LLM, including historical context and any previous `writerFeedback`.
       3. Turn 0 Orchestration: If it's the first turn (`chapter_count === 0`), it calls `orchestrator.runTurn0` to get initial guidance and injects its `writerfeedback` into the prompt.
       4. LLM Call: Sends the assembled messages to the configured main LLM via `openrouterRequest`.
       5. Result: Returns the generated story text and any `orchestratorFeedback` (specifically from Turn 0) to the caller for further processing (e.g., VN transformation and saving). (Visual Novel Transformer Orchestrator)

  26. modules/orchestrator.js
  * Purpose: This module acts as a high-level story analyst or "director". It is designed to run periodically (e.g., every 4 chapters) 
     to review the recent story progress and generate structured feedback. This prevents narrative drift and improves long-term coherence.
     It searches backwards through the properties array (provided by chaptermanagement) to find its own last 'director's feedback'.
  * Main Functions:
      * runTurn0(lorebook, initialPremise): The proactive orchestrator. Called at the very beginning of a new story (Turn 0) to provide initial guidance based on the lorebook and the user's first prompt.
      * runPeriodic(chapters, properties): The reactive orchestrator. Called periodically (e.g., every few chapters) to review recent story progress and generate structured feedback. It takes the full chapter history as input.
      * createOrchestratorTurn0Prompt(lorebook, initialPremise): Constructs the detailed prompt for the Turn 0 orchestrator.
      * createOrchestratorPrompt(recentHistory, lastSelfFeedback): Constructs the detailed prompt for the periodic orchestrator, including the recent story chapters and its own previous notes.
      * CONFIG: The configuration object itself, directly exported for read access by other modules.
  * Modularity: Excellent. It is a stateless, pure-functional module. It receives data, processes it via an API call, and returns a result without causing any side effects (like writing to files).
  * Dependencies: utils.js (for openrouterRequest, readSettings).
  * Flow Analysis:
      * `runTurn0`: Triggered by `narrativeEngine.js` if `chapter_count === 0`. It receives the `lorebook` and the `initialPremise` (user's very first prompt). Its `writerfeedback` is injected into the `narrativeEngine`'s prompt.
      * `runPeriodic`: Triggered by `vnmanager_simple.js` during the VN transformation pipeline. It receives the full chapter history (chapters text and properties objects). It finds its own last `directorfeedback` from the history for context. It generates new feedback (`writerfeedback` and `directorfeedback`). This feedback is returned up to `vnmanager`, then to `main.js`, then to the renderer. The renderer then includes this feedback in the `auxobj` when calling `save-message-chapter`, permanently associating it with the current story turn.

  10. modules/prompt_builder.js
  The system uses a hierarchical approach to context management, aiming to provide the LLM with relevant
  information while keeping the overall context size manageable. This is achieved by segmenting the story
  into different levels of detail: Fulltext, Summary, and Synopsis, and then dynamically injecting relevant
  memories using RAG.

  Here's a breakdown of the approach:

  A. Hierarchical Context Levels:

   * Fulltext (6 chapters): This is the most detailed level, containing the raw, complete text of the most
     recent chapters. This provides the LLM with immediate, high-fidelity context for the ongoing narrative.
   * Summary (12 chapters): This level contains summarized versions of chapters older than the fulltext
     chapters. Summaries are shorter and capture the main events and key information, reducing token count
     while retaining important context.
   * Synopsis (40+ chapters and growing): This is the highest-level abstraction, containing very concise
     summaries or key points of the oldest chapters. The synopsis is designed to be "infinitely growing" in
     terms of the number of chapters it covers, meaning it will always attempt to include a high-level
     overview of the entire story history. This prevents the LLM from "forgetting" earlier events, even if
     they are far back in the narrative.

  B. Retrieval Augmented Generation (RAG) for Dynamic Memory Injection:

  The core of this approach is the dynamic injection of relevant memories into the synopsis using RAG.

   * "Auto" Chapters and RAG: The "auto" chapters (which are your synopsis chapters) are ingested into a RAG
     system (likely a vector store). This means that the content of these synopsis chapters is converted into
     numerical representations (embeddings) that can be quickly searched for semantic similarity.
   * Search String: When a new user prompt is received, a "search string" is created. This search string
     combines the content of the very last chapter (from the fulltext) and the new user prompt. This ensures
     that the RAG search is highly relevant to the immediate conversation and recent events.
   * Limited RAG Results: The RAG system is configured to return a limited number of top results (e.g., "top
     10 results"). These results are the most semantically similar memories from the synopsis chapters to the
     current search string.
   * Slotted Memories: The retrieved RAG results are not simply appended as a loose memory. Instead, they are
     "slotted" directly into the synopsis. This means that if a RAG result is relevant to Chapter 3 and
     Chapter 5, those specific memories will appear directly below the synopsis entry for Chapter 3 and
     Chapter 5, respectively. This provides the LLM with a clear understanding of when and where these
     memories occurred in the timeline of the story.

  Example of Context Structure:

    1 [Synopsis:]
    2 Chapter 1: The hero arrived at the city.
    3 Chapter 2: The hero bought a sword.
    4 Chapter 3: The hero met Fiona.
    5 Fiona is an elven blacksmith who gave the hero a quest.  <-- RAG memory for Chapter 3
    6 The hero learned how to cook ork pasta.                 <-- RAG memory for Chapter 3
    7 Chapter 4: The hero leaves the city.
    8 Chapter 5: The hero meets an archer.
    9 An archer saves the hero's life by shooting a monster from far away. <-- RAG memory for Chapter
      5
   10 ...
   11
   12 [Summary]: 10 chapters (summarized content)
   13 [Fulltext]: 5 chapters (full, raw text)

  Benefits of this Approach:

   * Manages Context Window Limits: By using a hierarchical approach (fulltext -> summary -> synopsis) and
     RAG, the system can provide the LLM with a broad overview of the entire story without exceeding the LLM's
      context window limits.
   * Maintains Long-Term Coherence: The "infinitely growing" synopsis ensures that the LLM always has access
     to the entire story's progression, preventing narrative inconsistencies or "forgetting" past events.
   * Dynamic Relevance: RAG ensures that only the most relevant memories from the vast synopsis are injected
     into the prompt, keeping the context focused and reducing noise.
   * Temporal Awareness: By "slotting" RAG memories directly into their respective chapters within the
     synopsis, the LLM gains a better understanding of the temporal context of those memories, making them
     more useful for generating coherent and logical responses.
   * Adaptability: The dynamic nature of RAG means that the specific memories injected will change with each
     turn, adapting to the evolving conversation and narrative needs.

   In essence, this approach allows the LLM to have both a detailed short-term memory (fulltext), a condensed
   medium-term memory (summary), and a searchable, dynamically relevant long-term memory (synopsis + RAG),
   all working together to maintain narrative coherence and quality.

   * Purpose: Constructs the final prompt that is sent to the LLM. This involves reading selected files,
     potentially summarizing them or retrieving relevant memories (RAG), and combining them with the user's
     prompt. It handles different file processing modes (full, summary, auto, chat).
     It is also responsible for closing the feedback loop from the orchestrator module by injecting its feedback 
     into the prompt for the main writing LLM.
   * Design Philosophy: Granular Context Management
     This module's ability to process static files in different modes (`fulltext`, `summary`, `auto`) is a core feature that elevates the system's intelligence. It applies the same sophisticated context management from the chronological story history to the static world bible, giving the user granular control over their "context budget":
       * **Fulltext:** For short, critical files (like core system directives or character personality summaries) that must *always* be included in the prompt verbatim.
       * **Summary:** For long lore documents where the user wants the key points included without flooding the context window. The system does the heavy lifting of summarizing it for them.
       * **Auto (RAG):** This is the key for scalability. For massive files (like a bestiary, a list of locations, or extensive historical documents), the user can trust the system to intelligently pull only the specific, relevant chunks based on the current prompt, using a dedicated vector store for the static lore.
   * Main Functions:
       * injectToPlayerPrompt(prompt): Injects text into the user's prompt to guide the LLM's response, emphasizing player agency and realistic outcomes.
       * createSearchString(lastFileContent, prompt): Creates a search string for memory retrieval, combining recent content and the user's prompt.
       * groupFilesByFolder(files, rootDirectory): Groups files by their top-level folder for structured processing.
       * processVirtualChapters(chapters, startIdx): Creates virtual file objects from chronological chapter data (synopsis, summary, full text) for RAG processing.
       * retrieveAndGroupMemories(autoChapters, searchString, projectName, searchStringHandler): Ingests 'auto' chapters into the RAG system, performs a memory search, and groups retrieved memories by chapter ID.
       * processAllLoreFiles(allFiles, rootDirectory, memoryMap): Processes all lore files (static and virtual), generating summaries/synopses and injecting relevant RAG memories.
       * processLoreFolder(filePaths, rootDirectory, chapterHistory, prompt, searchStringHandler): Orchestrates the processing of a lore folder, including virtual chapters and RAG.
       * buildMessages(files, prompt, rootDirectory, getChatFilePathFromRenderer, chaptermanagementInstance, searchStringHandler, isObsidianMode): The core function that orchestrates:
           * Utilizes an extensible `fileProcessingStrategies` object to handle different file modes (e.g., `summary`, `charsheet`, `full`).
           * Centralized retrieval of chapter history and latest writer feedback from the orchestrator.
           * Static lore RAG pre-processing: Ingests and retrieves relevant memories from static 'auto' files based on the user's prompt.
           * Processing of all static files (System, World Building, other lore) based on their type (summary, fulltext, auto).
           * Processing of chronological chat history (virtual chapters) including RAG for 'auto' chapters.
           * Assembling the final message array for the LLM, injecting static RAG results (within `<retrieved_world_knowledge>` tags) and orchestrator feedback into the system prompt.
   * Modularity: Good. It orchestrates content gathering but delegates summarization and memory retrieval to
     memorymanagement.
   * Dependencies: path, fs/promises, utils.js (for Logger, generateHash, replaceAll, naturalSort), memorymanagement.js.
   * Flow Analysis: This is a critical step in preparing the LLM input. It is called by `narrativeEngine.js` to dynamically build the prompt based on user file selections and the RAG system.
     This module plays a key role in the story's quality control. By injecting the orchestrator's feedback, 
     it ensures that the creative LLM's output is continuously guided and refined, preventing plot holes 
     and improving narrative consistency.