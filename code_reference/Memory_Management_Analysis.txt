  11. modules/memorymanagement.js (Memory Management Facade)


   * Purpose: This module provides a public API facade for the Retrieval Augmented Generation (RAG) system. It
      orchestrates the processing, storage, and retrieval of "memories" (chunks of text) using a vector store
     (ChromaDB via vector_store_manager) and LLM-based summarization and relevance scoring.
   * Main Functions:
       * configure(): Sets up API keys, models, and debug mode.
       * updateChapter(chapterText, chapterId, metadata, projectName): Processes or updates a chapter, storing its content as embeddings in the 'chapters' vector store. Alias for processChapter.
       * processChapter(chapterText, chapterId, metadata, projectName): Processes a chapter, storing its content as embeddings. (Alias for updateChapter)
       * processStaticFile(fileContent, filePath, projectName): Processes a static lore file, storing it in the 'static_lore' vector store.
       * memoryProcessor(currentContext, options): The main entry point for retrieving context. Requires `options.storeType` ('chapters' or 'static_lore') to specify which vector store to query. It uses memory_retriever and chapter_processor.
       * storePlainTextChunks(text, projectName, metadata): Stores plain text chunks directly into the vector store.
       * searchPlainTextChunks(query, projectName, topK): Searches stored plain text chunks.
       * deletePlainTextChunksByMetadata(metadataFilter, projectName): Deletes plain text chunks based on a metadata filter.
       * SummarizationService (object): Exposes generateSummary, generateSynopsis, retrieveSummary, retrieveSynopsis from summarization_service.js.
       * enableLLMRelevanceScoring(), scoreLLMRelevance(), applyLLMRelevanceScoring(): Functions to control and apply LLM-based relevance scoring during retrieval.
       * initVectorStore(projectName, storeType): Initializes and returns a vector store instance.
       * isChapterProcessed(chapterId, contentHash, projectName): Checks if a chapter has already been processed.
       * generateContentHash(content): Generates a content hash.
       * ensureOverview(): Deprecated. Overviews are now generated during `processChapter`.
       * updateDocumentMetadata(): Deprecated.
       * getProjectMetadata(): Stub function.
   * Modularity: Excellent. It acts as a clean facade, abstracting the complexity of its internal utils
     modules.
   * Dependencies: langchain/document, dotenv, fs, path, utils.js,
     memorymanagement/utils/vector_store_manager.js, memorymanagement/utils/chapter_processor.js,
     memorymanagement/utils/memory_retriever.js, memorymanagement/utils/summarization_service.js,
     memorymanagement/utils/chapter_indexer.js.
   * Flow Analysis:
       1. Configuration: obsidianmode.js calls memorymanagement.configure() at startup.
       2. Chapter Processing: chapter_processor.js (via processChapter or updateChapter) is called to chunk and embed text,
          storing it in the vector store. This happens when files are processed in "auto" mode or when chat history is saved.
       3. Static File Processing: `processStaticFile` is called to ingest static lore into a dedicated vector store.
       4. Memory Retrieval (RAG): prompt_builder.js calls memorymanagement.memoryProcessor() with the user's
          query and a `storeType`.
           * memoryProcessor() calls memory_retriever.retrieve() to find relevant chunks from the specified vector
             store.
           * It then calls memory_retriever.assembleContext() to format these chunks into a coherent context
             string for the LLM.
           * LLM relevance scoring can be enabled to refine retrieval.

       5. Memory Inspection: The `main.js` process can call `memorymanagement.inspectStore()` in response to a request from a debug UI. This function uses `vector_store_manager` to get a direct handle on a ChromaDB collection and retrieves its entire contents, which are then formatted and sent to the UI for display.

   13. modules/memorymanagement/utils/chapter_processor.js


   * Purpose: Handles the processing of individual "chapters" (text blocks) for the RAG system. It chunks the
     text, generates an overview and chunk-level summaries using an LLM, and stores these chunks along with
     metadata in the vector store. It also checks if a chapter has already been processed.
   * Main Functions:
       * isProcessed(chapterId, contentHash, projectName): Checks if a chapter (identified by ID and content hash) has already been stored in the vector store.
       * processChapter(chapterText, chapterId, metadata, projectName, config): The core function:
           * Generates a content hash for the chapter.
           * Checks if the chapter has already been processed with identical content.
           * Generates an overall "overview" for the chapter using an LLM (via `openrouterRequest`).
           * Splits the chapter into smaller chunks using `RecursiveCharacterTextSplitter`.
           * Generates a "summary" for each chunk using an LLM (via `openrouterRequest`).
           * Adds all chunks with their metadata (chapterId, contentHash, overview, chunkPosition, summary) to
              the vector store.
   * Modularity: High. Focused on the chapter processing logic.
   * Dependencies: langchain/text_splitter, utils.js (for Logger, generateHash, openrouterRequest), memorymanagement/utils/vector_store_manager.js.
   * Flow Analysis: Called by `memorymanagement.processChapter()` (which is called by `prompt_builder` when
     processing "auto" files or saving chat history). This is where the actual chunking and embedding for RAG happens.


   12. modules/memorymanagement/utils/chapter_indexer.js


   * Purpose: Indexes chapter synopses for high-level relevance searching and ranks chapters based on a query.
   * Main Functions:
       * indexChapter(chapterId, synopsis, projectName): Stores a chapter's synopsis in a dedicated vector store (ChromaDB) for quick relevance lookups. It uses the `chapterId` as a unique identifier to allow for updates.
       * rankChaptersByRelevance(query, projectName): Performs a similarity search against the indexed chapter synopses and returns a sorted list of chapters by their relevance score to the given query.
   * Modularity: High. Focused on high-level chapter indexing and ranking for RAG.
   * Dependencies: langchain/document, utils.js, memorymanagement/utils/vector_store_manager.js.
   * Flow Analysis: Used by `memorymanagement.js` (specifically `memoryProcessor`) to help identify the most relevant chapters for context retrieval, especially for chronological chat history.


  13. modules/memorymanagement/utils/memory_retriever.js


   * Purpose: Responsible for retrieving relevant text chunks from the vector store based on a query and
     assembling them into a final context string for the LLM. It includes logic for LLM-based relevance
     scoring.
   * Main Functions:
       * setLlmRelevanceScoring(enable): Configures whether LLM-based relevance scoring is enabled.
       * setScoringModel(model): Sets the LLM model to be used for relevance scoring.
       * _scoreLLMRelevance(queryContext, chunkContent): Uses an LLM to score the relevance of a chunk to a query (internal helper).
       * _applyLLMRelevanceScoring(results, queryContext): Applies LLM-based scoring to search results and re-sorts them (internal helper).
       * retrieve(currentContext, options, config): Performs similarity search in the vector store, applies filters (including `autoChapterIds`), and optionally uses LLM
         relevance scoring to select the most relevant full text and summary chunks. It uses `config` for `TOP_K_FULL`, `TOP_K_SUMMARY`, `MAX_CONTEXT_TOKENS`, and `INITIAL_MULTIPLIER`.
       * assembleContext(relevantMemories, systemPrompt, debugMode): Formats the retrieved full text and summary chunks into a single, structured
         string, grouped by chapter, with overviews. Can log the assembled context to a file in debug mode.
   * Modularity: High. Focused on retrieval and context assembly.
   * Dependencies: utils.js (for Logger, openrouterRequest, settings), memorymanagement/utils/vector_store_manager.js.
   * Flow Analysis: Called by `memorymanagement.memoryProcessor()`. This is the "R" (Retrieval) part of RAG. It
     queries the vector store and prepares the context for the LLM.


  15. modules/memorymanagement/utils/summarization_service.js


   * Purpose: Provides functions for generating and retrieving summaries, synopses, and character sheets of text using an LLM. It
      caches these generated outputs in a separate ChromaDB collection to avoid redundant LLM calls. It also integrates with the `fact_manager` for character personality data.
   * Main Functions:
       * `_getSummaryVectorStore(projectName)`: Gets a specific vector store for summaries/synopses for a given project.
       * `_storeInChroma(content, output, type, projectName)`: Stores generated summaries/synopses in ChromaDB, associating them with the original content hash, type (summary/synopsis), and project name.
       * `_retrieveFromChroma(content, type, projectName)`: Retrieves cached summaries/synopses from ChromaDB based on content hash, type, and project name.
       * `generateSummary(content, projectName, model)`: Generates a detailed summary using an LLM (using `summaryPrompt` and the specified `model`). It first checks for a cached version.
       * `generateSynopsis(content, projectName, model)`: Generates an ultra-condensed synopsis using an LLM (using `synopsisPrompt` and the specified `model`). It first checks for a cached version.
       * `generateCharacterSheet(content, projectName, model)`: Generates a character sheet using an LLM (using `characterSheetPrompt` and the specified `model`). It first checks for a cached version. After generating the sheet, it calls `fact_manager.extractAndStoreInitialPersonalityVectors` to extract and store personality traits from the generated sheet. It then calls `fact_manager.synthesizeCharacterPersonality` to programmatically generate a natural language summary of the character's personality, which is appended to the character sheet.
       * `processVectors(cached, projectName)`: A helper function used by `generateCharacterSheet` to retrieve and append the synthesized character personality from `fact_manager.js` to the generated character sheet.
   * Modularity: High. Focused on summarization, caching, and integration with character personality management.
   * Dependencies: `langchain/document`, `utils.js` (for Logger, generateHash, openrouterRequest, readFileSync), `memorymanagement/utils/vector_store_manager.js`, `memorymanagement/utils/fact_manager.js`.
   * Flow Analysis: Used by `prompt_builder.js` when files are set to "summary" mode. It leverages caching to optimize LLM usage. The `generateCharacterSheet` function now plays a crucial role in populating the `fact_manager`'s database with character personality data and enriching the character sheet with a synthesized personality summary.


  16. modules/memorymanagement/utils/vector_store_manager.js


   * Purpose: Manages the initialization and access to ChromaDB vector stores. It ensures that a single
     instance of a vector store (for a given project and collection suffix) is reused, and handles connection
     to the Ollama embeddings model.
   * Main Functions:
       * getCollectionName(projectName, suffix): Generates a sanitized collection name for ChromaDB based on the project name and a suffix.
       * getStore(projectName, collectionSuffix): The main function to get or create a ChromaDB vector store instance. It uses
         OllamaEmbeddings for embedding text. It ensures that a single instance of a vector store is reused for a given collection name.
       * getChapterIndexStore(projectName): Returns a specific vector store instance for chapter indexing, using a distinct collection suffix.
   * Modularity: High. Encapsulates all vector store interaction logic.
   * Dependencies: @langchain/community/vectorstores/chroma, @langchain/ollama, utils.js (for Logger, readSettings).
   * Flow Analysis: This is a foundational component for the RAG system. It's called by `chapter_processor.js`,
     `memory_retriever.js`, `summarization_service.js`, and `chapter_indexer.js` to interact with the vector database.