Configuration and Utilities Analysis:

This document outlines the functionality and implementation details of the `modules/utils.js` file and the `settings.json` configuration.

### 5. modules/utils.js (Shared Utilities)

*   **Purpose:** Provides a collection of common utility functions used across various modules in the project. This includes logging, file system operations, string manipulation, hashing, and progress reporting. It also handles the loading and exposure of application settings from `settings.json`. LLM-related API requests have been refactored into `modules/llm.js` for multi-provider support.
*   **Main Functions:**
    *   `loadSettings()`: Loads application settings from `settings.json` at startup.
    *   `Logger` (object with `log`, `error`): A custom logging utility that respects the `debug` setting from `settings.json`.
    *   `TurnLogger`: A module for managing turn-based log files, ensuring logs are written asynchronously and can be retrieved.
        *   `startNewTurnLog(projectName)`: Initializes a new log file for a turn within a project-specific subdirectory (`logs/<projectName>/`).
        *   `log(title, content)`: Queues a log entry for the current turn.
        *   `getCurrentLogPath()`: Returns the path of the current log file.
        *   `getLatestLogPath()`: Finds the path of the most recent log file.
        *   `readLatestLog()`: Reads and parses the content of the most recent log file.
    *   `readFileSync(relPath)`: Reads a file synchronously from the project root.
    *   `ensureDirectoryExists(dirPath)`: Asynchronously creates directories if they don't exist.
    *   `ensureDirectoryExistsSync(dirPath)`: Synchronously creates directories if they don't exist.
    *   `listFilesRecursive(dir, validExtensions)`: Recursively lists files in a directory with specific extensions.
    *   `appendToFile(filePath, content)`: Appends content to a file.
    *   `replaceAll(str, search, replacement)`: Replaces all occurrences of a substring in a string.
    *   `generateHash(content, algorithm)`: Generates a hash for the given content.
    *   `normalizeText(text)`: Normalizes text for comparison (lowercase, replaces spaces with underscores).
    *   `getFilename(filepath)`: Extracts the filename from a full path.
    *   `reportProgress(socket, { step, totalSteps, status, startTime })`: Reports progress via a socket.
    *   `naturalSort(a, b)`: Natural sort comparison function for file/folder names.
*   **Modularity:** Excellent. It's a pure utility module with no external dependencies beyond Node.js built-ins and `axios`. It's designed to be imported and used by other modules without creating circular dependencies.
*   **Dependencies:** `fs/promises`, `fs`, `path`, `crypto`, `axios`.
*   **Flow Analysis:** Provides foundational services. `Logger` for debugging, and file system functions for data persistence. The `TurnLogger` is essential for debugging and understanding the flow of the application by logging turn-based data. LLM interactions are now handled by `modules/llm.js`.

### settings.json (Application Configuration)

*   **Purpose:** This JSON file serves as the central configuration hub for the entire application. It defines various parameters that control the behavior of different modules, including API keys, model selections, LLM parameters, and feature toggles. With the introduction of `modules/llm.js` and multi-provider support, `settings.json` now centralizes the configuration for various LLM services.
*   **Key Sections and Their Purpose:**
    *   `debug`: Boolean. If `true`, enables verbose logging across the application.
    *   `sanitizeResponses`: Configuration for sanitizing LLM responses.
        *   `enabled`: Boolean. If `true`, enables the regex-based response sanitizer.
    *   `providers`: An object configuring different LLM providers and their associated models. Each key represents a provider name (e.g., `openrouter`, `openai`, `anthropic`, `deepseek`, `gemini`).
        *   Each provider object contains its `apiKey`, `url` (if applicable), and defines logical model tiers (e.g., `thinkingmodel`, `highendmodel`, `mediumendmodel`, `lowendmodel`) that map to specific model IDs for that provider.
    *   `writer`: Configuration for the main narrative writing LLM.
        *   `model`: Specifies which logical model tier (e.g., `highendmodel`) to use for writing.
        *   `provider`: The LLM provider to use (e.g., `openrouter`).
        *   `llm_params`: Parameters passed to the LLM for text generation (e.g., `temperature`, `max_tokens`, `extra.reasoning`).
        *   `retries`: Number of retries for LLM calls.
        *   `timeout`: Timeout for LLM calls in milliseconds.
    *   `summarizer`: Configuration for summarization models.
        *   `summary_model`: Logical model for general summaries.
        *   `summary_provider`: Provider for general summaries.
        *   `synopsis_model`: Logical model for generating synopses.
        *   `synopsis_provider`: Provider for generating synopses.
        *   `character_sheet_model`: Logical model for extracting character sheet information.
        *   `character_sheet_provider`: Provider for extracting character sheet information.
    *   `orchestrator`: Configuration for the orchestrator module.
        *   `model`: Specifies which logical model tier (e.g., `thinkingmodel`) to use for orchestrator tasks.
        *   `provider`: The LLM provider to use.
        *   `llm_params`: Parameters for the orchestrator LLM.
        *   `chapters_to_review`: Number of recent chapters the orchestrator should review.
        *   `enabled`: Boolean to enable/disable the orchestrator.
        *   `retries`: Number of retries for LLM calls.
        *   `timeout`: Timeout for LLM calls in milliseconds.
    *   `chroma`: Connection details for the ChromaDB vector database (for Retrieval Augmented Generation).
        *   `url`: The URL where the ChromaDB server is running. **If the ChromaDB server is not running, RAG and summarization features will gracefully degrade.**
    *   `ollama`: Connection to a local Ollama instance for generating text embeddings.
        *   `model`: The Ollama model to use for generating embeddings (e.g., `nomic-embed-text`).
        *   `provider`: The LLM provider for Ollama (e.g., `openrouter`).
        *   `baseUrl`: The base URL for the Ollama server. **If the Ollama server is not running, RAG and summarization features will gracefully degrade.**
    *   `assetSelector`: Configuration for selecting visual novel assets (backgrounds, OSTs).
        *   `enableOST`, `enableBackground`: Booleans to enable/disable OST and background selection.
        *   `background_model`, `ost_model`: Logical models to use for LLM-based asset selection.
        *   `background_provider`, `ost_provider`: Providers for asset selection.
        *   `background_params`, `ost_params`: LLM parameters for asset selection, including `retries` and `timeout`.
    *   `enableTTS`: Boolean. If `true`, enables Text-to-Speech generation.
    *   `ttsApiEndpoint`: The endpoint for the external TTS API.
    *   `emotionClassifier`: Configuration for character emotion classification.
        *   `enabled`: Boolean to enable/disable emotion classification.
        *   `model`: Logical model to use for emotion classification.
        *   `provider`: The LLM provider to use.
        *   `llm_params`: LLM parameters for emotion classification, including `retries` and `timeout`.
    *   `useUncensorifyPrompt`: Boolean. If `true`, uses an "uncensorify" prompt for dialogue transformation.
    *   `dialogue_processor`: Configuration for the dialogue processing module.
        *   `model`: Logical model to use for dialogue transformation.
        *   `provider`: The LLM provider to use.
        *   `llm_params`: LLM parameters for dialogue transformation, including `retries` and `timeout`.
        *   `enabled`: Boolean to enable/disable dialogue processing.
    *   `memoryRetriever`: Configuration for the LLM used for chunking and retrieving relevant memories from the vector database.
        *   `model`: Logical model to use.
        *   `provider`: The LLM provider to use.
        *   `retries`: Number of retries for LLM calls.
        *   `timeout`: Timeout for LLM calls in milliseconds.
        *   `llm_params`: LLM parameters for memory retrieval.
    *   `staticLore`: Configuration for static lore Retrieval Augmented Generation (RAG).
        *   `top_k`: Number of top results to retrieve from files set as "Auto".
    *   `player_choice_generator`: Configuration for generating player choices.
        *   `model`: Logical model to use for player choice generation.
        *   `provider`: The LLM provider to use.
        *   `llm_params`: LLM parameters for player choice generation, including `retries` and `timeout`.
        *   `enabled`: Boolean to enable/disable player choice generation.
    *   `fact_manager`: Configuration for the fact management system.
        *   `enabled`: Boolean to enable/disable the fact manager.
        *   `facts`: Configuration for fact extraction and management.
            *   `model`: Logical model for fact-related operations.
            *   `provider`: The LLM provider to use.
            *   `llm_params`: LLM parameters for fact operations, including `retries` and `timeout`.
            *   `enabled`: Boolean to enable/disable fact processing.
        *   `world_state_synthesizer`: Configuration for synthesizing world state.
            *   `model`: Logical model to use.
            *   `provider`: The LLM provider to use.
            *   `llm_params`: LLM parameters, including `retries` and `timeout`.
            *   `enabled`: Boolean to enable/disable.
        *   `personality`: Configuration for personality vector extraction.
            *   `model`: Logical model for personality-related operations.
            *   `provider`: The LLM provider to use.
            *   `llm_params`: LLM parameters for personality operations, including `retries` and `timeout`.
            *   `enabled`: Boolean to enable/disable personality processing.
        *   `personality_vector_extractor`: Specific configuration for the personality vector extraction.
            *   `model`: Logical model for personality vector extraction.
            *   `provider`: The LLM provider to use.
            *   `llm_params`: LLM parameters for personality vector extraction, including `retries` and `timeout`.
            *   `enabled`: Boolean to enable/disable personality vector extraction.
    *   `modelCosts`: Provides pricing information for various LLM models across different providers.
        *   Each key represents a provider name (e.g., `openrouter`, `openai`, `anthropic`, `deepseek`, `gemini`).
        *   Under each provider, keys represent model names (e.g., `deepseek/deepseek-chat-v3.1`).
        *   `input`: Cost per 1M input tokens.
        *   `output`: Cost per 1M output tokens.
        *   `unit`: Unit of cost (e.g., "USD per 1M tokens").
        *   `context`: Context window size of the model.
*   **Impact on Application Behavior:** `settings.json` directly influences the application's behavior, from which LLM models are used for different tasks to whether TTS is enabled or how verbose the logging is. Changes to this file allow for easy customization and tuning of the application without modifying code.
*   **Relationship with `modules/utils.js` and `modules/llm.js`:** The `modules/utils.js` file is responsible for loading these settings at application startup, making them accessible to other modules via the `settings` object. The `modules/llm.js` then uses these settings to initialize and manage LLM interactions with various providers.
