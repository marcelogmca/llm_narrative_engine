16. modules/chaptermanagement.js

   * Purpose: Manages the persistent storage and retrieval of `TurnContext` instances as "chapters" in a SQLite database. It can append `TurnContext` instances, retrieve all instances, and categorize them into "full," "summary," and "synopsis" chapters based on length and recency. **It now includes a robust check for `TurnContext` instances during appending to prevent issues with circular dependencies.**
     **Crucially, this module stores the comprehensive `TurnContext` object, allowing for the preservation of rich, turn-specific metadata, including feedback from other systems (like the orchestrator), game state snapshots, and character relationship scores. It forms the persistent, on-disk "memory" of the story's evolution, ensuring that every aspect of a turn is meticulously recorded and retrievable.**
   * Database Schema: The chat history is stored in a `chat_turns` table within a SQLite database (e.g., `Chat.db` or dynamically named .db files).
     The table schema is:
       * `id` (INTEGER PRIMARY KEY AUTOINCREMENT)
       * `project_name` (TEXT NOT NULL): The name of the project this chat belongs to.
       * `turn_number` (INTEGER NOT NULL): The sequential number of the chat turn within the project.
       * `turn_context_snapshot` (TEXT NOT NULL): A stringified JSON object containing the serialized `TurnContext` instance.
       * `timestamp` (DATETIME DEFAULT CURRENT_TIMESTAMP): The time the turn was recorded.
   * Main Functions:
       * init(chatDbFullPath): Initializes the module by opening the SQLite database at the given full path and creating the `chat_turns` table if it doesn't exist. It also derives the `project_name` from the path.
       * `appendMessage(turnContextInstance)`: Appends a `TurnContext` instance to the database. **This function is now primarily called by `TurnContext.commit()`, ensuring that the saving process is integrated with the `TurnContext` lifecycle, and includes a robust check for `TurnContext` instances.**
       * getChapters(): Retrieves all `TurnContext` instances from the database for the current project, reconstructing them from their serialized form.
       * getChapterCount(): Retrieves the total number of messages (chapters) in the database for the current project.
       * retrieveDatedChapters(): Categorizes `TurnContext` instances into recent "full," older "summary," and oldest
         "synopsis" sections. This function is now exposed via `TurnContext.retrieveDatedChapters()`.
       * configure(newConfig): Updates module configuration (e.g., `FULL_CHAPTERS`, `SUMMARY_CHAPTERS`).
       * CONFIG: The configuration object itself, directly exported for read access by other modules.
   * Modularity: Good. Focused on chapter database management.
   * Dependencies: utils.js, sqlite, sqlite3, path, TurnContext.js.
   * Flow Analysis:
       1. Initialization: `main.js` (specifically `fileHandlers.saveFileConfig`) calls `chaptermanagement.init()` with the full path to the active chat database file (e.g., `projects/Test/3_Lore/Chat.db`).
       2. Message Saving: `TurnContext.commit()` now orchestrates the saving of the `TurnContext` instance by calling `chaptermanagement.appendMessage()`, ensuring a consistent and complete record of each turn.
       3. Chapter Retrieval: Modules like `prompt_builder.js` and `orchestrator.js` now call `turnContext.retrieveDatedChapters()` to get historical chat context for the LLM, including any orchestrator feedback stored within the `TurnContext` properties.

  17. modules/narrativeEngine.js (Narrative Generation Engine)


   * Purpose: This module is the central orchestrator for generating the core narrative text using LLMs. It integrates prompt building, orchestrator feedback, and handles the main LLM calls. **It now operates exclusively on a `TurnContext` object, which encapsulates all turn-specific data, simplifying its interface and internal logic, and is designed to gracefully handle cases where RAG or summarization services are unavailable.**
   * Main Functions:
       * generateNextChapter(turnContext): The primary function responsible for:
           * Receiving a `TurnContext` object, which contains all necessary input and **serves as the central data structure for populating processed data throughout the narrative generation process.**
           * Calling `prompt_builder.buildMessages(turnContext)` to construct the LLM prompt, incorporating selected files, user prompt, and historical chapter data (now accessed via `turnContext.retrieveDatedChapters()`).
           * Injecting `writerFeedback` from previous orchestrator runs into the prompt to guide the LLM.
           * Executing the Turn 0 orchestrator (`orchestrator.runTurn0`) if it's the beginning of a new story, and injecting its `writerfeedback`.
           * Making the final LLM call to OpenRouter to generate the next chapter's text.
           * **Populating the `turnContext.processed.narrativeEngine.writerResponse` and `turnContext.processed.orchestrator.feedback` fields directly within the `TurnContext` object, ensuring all generated data is encapsulated and accessible.**
       * configure(config): Sets up necessary dependencies like `chapterManagement` (though its direct use for retrieval is now minimized) and `searchStringHandler`.
   * Modularity: High. It orchestrates the narrative generation process, delegating prompt construction to `prompt_builder` and orchestrator logic to `orchestrator`. **Its strong reliance on `TurnContext` significantly simplifies its signature and internal state management, making it more robust and easier to maintain.**
   * Dependencies: utils.js (for Logger, openrouterRequest, readSettings), prompt_builder.js, fs/promises, obsidianmode/utils/cache_manager.js (for `baseCacheDir`), orchestrator.js, path.
   * Flow Analysis:
       1. Trigger: Called by `main.js` (from `fileHandlers.sendToLLM` for text generation tab, or `vnHandlers.generateVNTurn` for VN tab), passing a `TurnContext` object.
       2. Prompt Building: Calls `prompt_builder.buildMessages(turnContext)` to get the structured messages for the LLM, including historical context and any previous `writerFeedback`.
       3. Turn 0 Orchestration: If it's the first turn (`turnContext.turnNumber - 1 === 0`), it calls `orchestrator.runTurn0` to get initial guidance and injects its `writerfeedback` into the prompt.
       4. LLM Call: Sends the assembled messages to the configured main LLM via `openrouterRequest`.
       5. Result: Populates the `TurnContext` object with the generated story text and any `orchestratorFeedback` (specifically from Turn 0) for further processing (e.g., VN transformation and saving).

  26. modules/orchestrator.js
  * Purpose: This module acts as a high-level story analyst or "director". It is designed to run periodically (e.g., every 4 chapters)
     to review the recent story progress and generate structured feedback. This prevents narrative drift and improves long-term coherence.
     It now receives a `TurnContext` object, allowing it to access the full chapter history and find its own last 'director's feedback'.
  * Main Functions:
      * runTurn0(lorebook, initialPremise): The proactive orchestrator. Called at the very beginning of a new story (Turn 0) to provide initial guidance based on the lorebook and the user's first prompt.
      * runPeriodic(turnContext): The reactive orchestrator. Called periodically (e.g., every few chapters) to review recent story progress and generate structured feedback. It now receives a `TurnContext` object, from which it retrieves the full chapter history.
      * createOrchestratorTurn0Prompt(lorebook, initialPremise): Constructs the detailed prompt for the Turn 0 orchestrator.
      * createOrchestratorPrompt(recentHistory, lastSelfFeedback): Constructs the detailed prompt for the periodic orchestrator, including the recent story chapters and its own previous notes.
      * CONFIG: The configuration object itself, directly exported for read access by other modules.
   * Modularity: Excellent. It is a stateless, pure-functional module. It receives data (now encapsulated in `TurnContext`), processes it via an API call, and returns a result without causing any side effects (like writing to files).
   * Dependencies: utils.js (for openrouterRequest, readSettings).
   * Flow Analysis:
      * `runTurn0`: Triggered by `narrativeEngine.js` if `chapter_count === 0`. It receives the `lorebook` and the `initialPremise` (user's very first prompt). Its `writerfeedback` is injected into the `narrativeEngine`'s prompt.
      * `runPeriodic`: Triggered by `vnmanager.js` during the VN transformation pipeline. It now receives the `TurnContext` object. It uses `turnContext.retrieveDatedChapters()` to get the full chapter history. It finds its own last `directorfeedback` from the history for context. It generates new feedback (`writerfeedback` and `directorfeedback`). This feedback is returned up to `vnmanager`, then to `main.js`, then to the renderer. The renderer then includes this feedback in the `auxobj` when calling `save-message-chapter`, permanently associating it with the current story turn.

  10. modules/prompt_builder.js
  The system uses a hierarchical approach to context management, aiming to provide the LLM with relevant
  information while keeping the overall context size manageable. This is achieved by segmenting the story
  into different levels of detail: Fulltext, Summary, and Synopsis, and then dynamically injecting relevant
  memories using RAG.

  Here's a breakdown of the approach:

  A. Hierarchical Context Levels:

   * Fulltext (6 chapters): This is the most detailed level, containing the raw, complete text of the most
     recent chapters. This provides the LLM with immediate, high-fidelity context for the ongoing narrative.
   * Summary (12 chapters): This level contains summarized versions of chapters older than the fulltext
     chapters. Summaries are shorter and capture the main events and key information, reducing token count
     while retaining important context.
   * Synopsis (40+ chapters and growing): This is the highest-level abstraction, containing very concise
     summaries or key points of the oldest chapters. The synopsis is designed to be "infinitely growing" in
     terms of the number of chapters it covers, meaning it will always attempt to include a high-level
     overview of the entire story history. This prevents the LLM from "forgetting" earlier events, even if
     they are far back in the narrative.

  B. Retrieval Augmented Generation (RAG) for Dynamic Memory Injection:

  The core of this approach is the dynamic injection of relevant memories into the synopsis using RAG.

   * "Auto" Chapters and RAG: The "auto" chapters (which are your synopsis chapters) are ingested into a RAG
     system (likely a vector store). This means that the content of these synopsis chapters is converted into
     numerical representations (embeddings) that can be quickly searched for semantic similarity.
   * Search String: When a new user prompt is received, a "search string" is created. This search string
     combines the content of the very last chapter (from the fulltext) and the new user prompt. This ensures
     that the RAG search is highly relevant to the immediate conversation and recent events.
   * Limited RAG Results: The RAG system is configured to return a limited number of top results (e.g., "top
     10 results"). These results are the most semantically similar memories from the synopsis chapters to the
     current search string.
   * Slotted Memories: The retrieved RAG results are not simply appended as a loose memory. Instead, they are
     "slotted" directly into the synopsis. This means that if a RAG result is relevant to Chapter 3 and
     Chapter 5, those specific memories will appear directly below the synopsis entry for Chapter 3 and
     Chapter 5, respectively. This provides the LLM with a clear understanding of when and where these
     memories occurred in the timeline of the story.

  Example of Context Structure:

    1 [Synopsis:]
    2 Chapter 1: The hero arrived at the city.
    3 Chapter 2: The hero bought a sword.
    4 Chapter 3: The hero met Fiona.
    5 Fiona is an elven blacksmith who gave the hero a quest.  <-- RAG memory for Chapter 3
    6 The hero learned how to cook ork pasta.                 <-- RAG memory for Chapter 3
    7 Chapter 4: The hero leaves the city.
    8 Chapter 5: The hero meets an archer.
    9 An archer saves the hero's life by shooting a monster from far away. <-- RAG memory for Chapter
      5
   10 ...
   11
   12 [Summary]: 10 chapters (summarized content)
   13 [Fulltext]: 5 chapters (full, raw text)

  Benefits of this Approach:

   * Manages Context Window Limits: By using a hierarchical approach (fulltext -> summary -> synopsis) and
     RAG, the system can provide the LLM with a broad overview of the entire story without exceeding the LLM's
      context window limits.
   * Maintains Long-Term Coherence: The "infinitely growing" synopsis ensures that the LLM always has access
     to the entire story's progression, preventing narrative inconsistencies or "forgetting" past events.
   * Dynamic Relevance: RAG ensures that only the most relevant memories from the vast synopsis are injected
     into the prompt, keeping the context focused and reducing noise.
   * Temporal Awareness: By "slotting" RAG memories directly into their respective chapters within the
     synopsis, the LLM gains a better understanding of the temporal context of those memories, making them
     more useful for generating coherent and logical responses.
   * Adaptability: The dynamic nature of RAG means that the specific memories injected will change with each
     turn, adapting to the evolving conversation and narrative needs.

   In essence, this approach allows the LLM to have both a detailed short-term memory (fulltext), a condensed
   medium-term memory (summary), and a searchable, dynamically relevant long-term memory (synopsis + RAG),
   all working together to maintain narrative coherence and quality.

   * Purpose: Constructs the final prompt that is sent to the LLM. This involves reading selected files,
     potentially summarizing them or retrieving relevant memories (RAG), and combining them with the user's
     prompt. It handles different file processing modes (full, summary, auto, chat).
     It is also responsible for closing the feedback loop from the orchestrator module by injecting its feedback 
     into the prompt for the main writing LLM. **This module now operates seamlessly with the `TurnContext` object, leveraging its encapsulated data for efficient prompt construction.**
   * Design Philosophy: Granular Context Management
     This module's ability to process static files in different modes (`fulltext`, `summary`, `auto`) is a core feature that elevates the system's intelligence. It applies the same sophisticated context management from the chronological story history to the static world bible, giving the user granular control over their "context budget":
       * **Fulltext:** For short, critical files (like core system directives or character personality summaries) that must *always* be included in the prompt verbatim.
       * **Summary:** For long lore documents where the user wants the key points included without flooding the context window. The system does the heavy lifting of summarizing it for them.
       * **Auto (RAG):** This is the key for scalability. For massive files (like a bestiary, a list of locations, or extensive historical documents), the user can trust the system to intelligently pull only the specific, relevant chunks based on the current prompt, using a dedicated vector store for the static lore.
   * Main Functions:
       * injectToPlayerPrompt(prompt): Injects text into the user's prompt to guide the LLM's response, emphasizing player agency and realistic outcomes.
       * createSearchString(lastFileContent, prompt): Creates a search string for memory retrieval, combining recent content and the user's prompt.
       * groupFilesByFolder(files, rootDirectory): Groups files by their top-level folder for structured processing.
       * processVirtualChapters(chapters, startIdx): Creates virtual file objects from chronological chapter data (synopsis, summary, full text) for RAG processing.
       * retrieveAndGroupMemories(autoChapters, searchString, projectName, searchStringHandler): Ingests 'auto' chapters into the RAG system, performs a memory search, and groups retrieved memories by chapter ID.
       * processAllLoreFiles(allFiles, rootDirectory, memoryMap): Processes all lore files (static and virtual), generating summaries/synopses and injecting relevant RAG memories.
       * processLoreFolder(filePaths, rootDirectory, chapterHistory, prompt, searchStringHandler): Orchestrates the processing of a lore folder, including virtual chapters and RAG.
       * buildMessages(turnContext): The core function that orchestrates:
           * **Receives a `TurnContext` object, from which it extracts all necessary data (selected files, user prompt, project name, etc.), ensuring a single, consistent source of truth for prompt building.**
           * Utilizes an extensible `fileProcessingStrategies` object to handle different file modes (e.g., `summary`, `charsheet`, `full`).
           * Centralized retrieval of chapter history and latest writer feedback from the orchestrator, now accessed via `turnContext.retrieveDatedChapters()`.
           * Static lore RAG pre-processing: Ingests and retrieves relevant memories from static 'auto' files based on the user's prompt.
           * Processing of all static files (System, World Building, other lore) based on their type (summary, fulltext, auto).
           * Processing of chronological chat history (virtual chapters) including RAG for 'auto' chapters.
           * Assembling the final message array for the LLM, injecting static RAG results (within `<retrieved_world_knowledge>` tags) and orchestrator feedback into the system prompt.
           * **Populates `turnContext.processed.promptBuilder.messages`, `turnContext.processed.promptBuilder.lorebook`, and `turnContext.processed.promptBuilder.writerFeedback` directly within the `TurnContext` object, making the prompt building process transparent and its outputs easily accessible.**
   * Modularity: Good. It orchestrates content gathering but delegates summarization and memory retrieval to
     memorymanagement. **Its reliance on `TurnContext` significantly simplifies its signature and internal state management, contributing to a cleaner and more maintainable codebase.**
   * Dependencies: path, fs/promises, utils.js (for Logger, generateHash, replaceAll, naturalSort), memorymanagement.js.
   * Flow Analysis: This is a critical step in preparing the LLM input. It is called by `narrativeEngine.js` to dynamically build the prompt based on user file selections and the RAG system.
     This module plays a key role in the story's quality control. By injecting the orchestrator's feedback,
     it ensures that the creative LLM's output is continuously guided and refined, preventing plot holes
     and improving narrative consistency.